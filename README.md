# ğŸ  House Value Prediction App

<div align="center">

![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)
![Scikit-Learn](https://img.shields.io/badge/Scikit--Learn-Latest-orange.svg)
![Streamlit](https://img.shields.io/badge/Streamlit-Latest-red.svg)
![License](https://img.shields.io/badge/License-MIT-green.svg)

**A complete end-to-end Machine Learning project for predicting California house values using Random Forest Regression**

[Live Demo](https://house-value-prediction-bysubodhit.streamlit.app/) | [Report Bug](https://github.com/Subodhitchouhan/House-Value-Prediction/issues) | [Request Feature](https://github.com/Subodhitchouhan/House-Value-Prediction/issues)

</div>

---

## ğŸ“‹ Table of Contents

- [Project Overview](#-project-overview)
- [Dataset](#-dataset)
- [Project Workflow](#-project-workflow)
  - [1. Data Collection](#1-data-collection)
  - [2. Exploratory Data Analysis (EDA)](#2-exploratory-data-analysis-eda)
  - [3. Data Preprocessing](#3-data-preprocessing)
  - [4. Data Visualization](#4-data-visualization)
  - [5. Feature Engineering](#5-feature-engineering)
  - [6. Model Building](#6-model-building)
  - [7. Model Evaluation](#7-model-evaluation)
  - [8. Model Persistence](#8-model-persistence)
  - [9. Web Application](#9-web-application)
- [Installation & Setup](#-installation--setup)
- [Usage](#-usage)
- [Project Structure](#-project-structure)
- [Technologies Used](#-technologies-used)
- [Results](#-results)
- [Future Improvements](#-future-improvements)
- [Contributing](#-contributing)
- [License](#-license)
- [Contact](#-contact)

---

## ğŸ¯ Project Overview

This project demonstrates a **complete Machine Learning pipeline** for predicting house values in California. It covers every step from raw data collection to deploying an interactive web application. The project uses the California Housing Dataset to build a Random Forest Regressor model that predicts median house values based on various features like location, housing characteristics, and demographics.

### Key Features:
- âœ… Comprehensive data preprocessing pipeline
- âœ… Stratified sampling for representative train-test split
- âœ… Advanced feature engineering and scaling
- âœ… Multiple ML models comparison (Linear Regression, Decision Tree, Random Forest)
- âœ… Cross-validation for robust model evaluation
- âœ… Model persistence using Joblib
- âœ… Interactive Streamlit web application
- âœ… Deployed on Hugging Face Spaces

---

## ğŸ“Š Dataset

### California Housing Dataset

The dataset contains information about housing in California from the 1990 census. Each row represents a **census block group** (the smallest geographical unit for which the U.S. Census Bureau publishes sample data).

**Dataset Specifications:**
- **Total Instances:** 20,640
- **Features:** 10 (9 input features + 1 target variable)
- **Missing Values:** Yes (in `total_bedrooms`)
- **Source:** California Housing Prices Dataset

### Features Description:

| Feature | Type | Description |
|---------|------|-------------|
| `longitude` | Continuous | Longitude coordinate of the block group |
| `latitude` | Continuous | Latitude coordinate of the block group |
| `housing_median_age` | Continuous | Median age of houses in the block group |
| `total_rooms` | Continuous | Total number of rooms in the block group |
| `total_bedrooms` | Continuous | Total number of bedrooms in the block group |
| `population` | Continuous | Total population in the block group |
| `households` | Continuous | Total number of households in the block group |
| `median_income` | Continuous | Median income of households (in tens of thousands of USD) |
| `ocean_proximity` | Categorical | Location category relative to the ocean |
| **`median_house_value`** | **Target** | **Median house value (in USD)** |

**Ocean Proximity Categories:**
- `<1H OCEAN` - Less than 1 hour from ocean
- `INLAND` - Inland location
- `NEAR OCEAN` - Near the ocean
- `NEAR BAY` - Near a bay
- `ISLAND` - On an island

---

## ğŸ”„ Project Workflow

### 1. Data Collection

The California Housing dataset is loaded using pandas from a CSV file:

```python
import pandas as pd
data = pd.read_csv("housing.csv")
```

**Initial Data Inspection:**
- Dataset shape: (20,640, 10)
- Data types: 9 numerical features, 1 categorical feature
- Missing values detected in `total_bedrooms` column

---

### 2. Exploratory Data Analysis (EDA)

EDA helps us understand the data structure, identify patterns, and detect anomalies.

#### 2.1 Basic Data Inspection

```python
# View first few rows
data.head()

# Dataset information
data.info()

# Statistical summary
data.describe()
```

**Key Findings:**

ğŸ“Š **Data Shape:**
- Total records: 20,640
- Total features: 10

ğŸ“Š **Data Types:**
- Numerical features: 9 (int64, float64)
- Categorical features: 1 (object)

ğŸ“Š **Missing Values:**
- `total_bedrooms`: 207 missing values (1% of data)

#### 2.2 Statistical Summary

**Numerical Features Statistics:**

| Statistic | longitude | latitude | housing_median_age | total_rooms | median_income |
|-----------|-----------|----------|--------------------|-------------|---------------|
| Mean | -119.57 | 35.63 | 28.64 | 2635.76 | 3.87 |
| Std | 2.00 | 2.14 | 12.59 | 2181.62 | 1.90 |
| Min | -124.35 | 32.54 | 1.00 | 2.00 | 0.50 |
| Max | -114.31 | 41.95 | 52.00 | 39320.00 | 15.00 |

**Key Insights:**
- âš ï¸ **Median income** is scaled (appears to be in tens of thousands)
- âš ï¸ **House values** are capped at $500,000 (potential data censoring)
- âš ï¸ Large variance in `total_rooms` suggests outliers
- âš ï¸ Housing age ranges from 1 to 52 years

#### 2.3 Distribution Analysis

```python
import matplotlib.pyplot as plt

# Create histograms for all numerical features
data.hist(bins=50, figsize=(12, 8))
plt.tight_layout()
plt.show()
```

![Data Distribution Histograms](A.DATA_DISTRIBUTION_HISTOGRAM.png)

**Distribution Insights:**

1. **Median Income:**
   - Right-skewed distribution
   - Most households have income between $20,000-$50,000
   - Few high-income outliers above $100,000

2. **Housing Median Age:**
   - Relatively uniform distribution
   - Spike at 52 years (possibly capped value)

3. **Median House Value (Target):**
   - Right-skewed distribution
   - Sharp spike at $500,000 (data ceiling)
   - Most houses valued between $100,000-$300,000

4. **Total Rooms & Bedrooms:**
   - Heavily right-skewed
   - Presence of outliers (very large properties)

5. **Geographical Features:**
   - Latitude and longitude show clustering patterns
   - Indicates geographical concentration of data points

---

### 3. Data Preprocessing

Data preprocessing is crucial for improving model performance and ensuring data quality.

#### 3.1 Creating Test Set with Stratified Sampling

**Why Stratified Sampling?**

Random sampling might not represent all income categories equally. Stratified sampling ensures the test set is representative of the overall population.

```python
import numpy as np
from sklearn.model_selection import StratifiedShuffleSplit

# Create income categories for stratification
data["income_cat"] = pd.cut(
    data["median_income"],
    bins=[0, 1.5, 3.0, 4.5, 6.0, np.inf],
    labels=[1, 2, 3, 4, 5]
)

# Perform stratified split
split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)

for train_index, test_index in split.split(data, data["income_cat"]):
    strat_train_set = data.loc[train_index]
    strat_test_set = data.loc[test_index]

# Remove temporary income category column
for set_ in (strat_train_set, strat_test_set):
    set_.drop("income_cat", axis=1, inplace=True)
```

**Results:**
- Training set: 16,512 samples (80%)
- Test set: 4,128 samples (20%)
- âœ… Income distribution preserved across both sets

#### 3.2 Handling Missing Values

```python
from sklearn.impute import SimpleImputer

# Strategy: Fill missing values with median
imputer = SimpleImputer(strategy="median")

# Apply only to numerical columns
housing_num = housing.select_dtypes(include=[np.number])
imputer.fit(housing_num)
X = imputer.transform(housing_num)
```

**Missing Value Treatment:**
- `total_bedrooms`: 207 missing â†’ Filled with median value (435.0)
- **Why median?** Less sensitive to outliers than mean

#### 3.3 Handling Categorical Features

The `ocean_proximity` feature needs encoding for ML algorithms.

**Option 1: Ordinal Encoding**
```python
from sklearn.preprocessing import OrdinalEncoder

ordinal_encoder = OrdinalEncoder()
housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)
```

**Option 2: One-Hot Encoding (Preferred)**
```python
from sklearn.preprocessing import OneHotEncoder

cat_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')
housing_cat_1hot = cat_encoder.fit_transform(housing_cat)
```

**Why One-Hot Encoding?**
- âœ… No ordinal relationship between ocean proximity categories
- âœ… Prevents model from assuming numerical order
- âœ… Creates 5 binary features (one per category)

#### 3.4 Feature Scaling

**Why Scale Features?**

Features like `total_rooms` (range: 2-39,320) and `median_income` (range: 0.5-15) have vastly different scales. This can bias ML algorithms.

```python
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
housing_scaled = scaler.fit_transform(housing_num)
```

**StandardScaler Formula:**
```
x_scaled = (x - mean) / std_deviation
```

**Result:**
- All features transformed to have mean = 0 and std = 1
- Ensures equal importance during model training

---

### 4. Data Visualization

Visualization reveals geographical patterns and correlations in the data.

#### 4.1 Geographical Scatter Plot

```python
import matplotlib.pyplot as plt

housing.plot(
    kind="scatter",
    x="longitude",
    y="latitude",
    alpha=0.4,
    s=housing["population"]/100,
    label="population",
    figsize=(10, 7),
    c="median_house_value",
    cmap=plt.get_cmap("jet"),
    colorbar=True,
)
plt.legend()
plt.show()
```

![California Housing Geographical Plot](B.GEOGRAPHICAL_SCATTERPLOT.png)

**Insights from Geographical Plot:**

ğŸ—ºï¸ **Location Patterns:**
- High-value properties clustered along the coast (especially Bay Area and Los Angeles)
- Lower values in inland areas
- Clear visualization of California's coastline

ğŸ™ï¸ **Population Density:**
- Larger circles indicate higher population
- Major cities clearly visible (San Francisco, Los Angeles, San Diego)

ğŸ’° **Price Patterns:**
- Red/Orange areas: High-value properties ($300k-$500k)
- Blue/Green areas: Lower-value properties ($50k-$150k)
- Strong correlation between coastal proximity and house value

#### 4.2 Correlation Matrix

```python
import seaborn as sns

# Calculate correlation matrix
corr_matrix = housing.corr()

# Plot heatmap
plt.figure(figsize=(12, 8))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0)
plt.title('Feature Correlation Matrix')
plt.show()
```

![Correlation Heatmap](C.correlational_matrix_heatmap.png)

**Key Correlations with Median House Value:**

| Feature | Correlation | Interpretation |
|---------|-------------|----------------|
| `median_income` | **+0.688** | ğŸ”¥ Strong positive - Higher income = Higher house value |
| `total_rooms` | +0.134 | Weak positive correlation |
| `housing_median_age` | +0.106 | Very weak positive correlation |
| `households` | +0.065 | Negligible correlation |
| `total_bedrooms` | +0.049 | Negligible correlation |
| `population` | -0.024 | Negligible negative correlation |
| `longitude` | -0.045 | Weak negative correlation |
| `latitude` | -0.144 | Moderate negative - Northern areas cheaper |

**ğŸ” Key Finding:**
- **Median income is the strongest predictor** of house value (correlation = 0.688)
- Geographical features show moderate importance
- Total rooms/bedrooms have surprisingly low correlation

#### 4.3 Scatter Plot: Income vs House Value

```python
housing.plot(
    kind="scatter",
    x="median_income",
    y="median_house_value",
    alpha=0.1
)
plt.show()
```

![Income vs House Value](D.scatter_plot_median_income_vs_meadian_house_value.png)

**Insights:**
- Clear positive linear relationship
- Horizontal line at $500k confirms data capping
- Some horizontal lines at other price points suggest artificial price caps
- Outliers exist but follow general trend

---

### 5. Feature Engineering

Creating new features that better represent the underlying patterns.

#### 5.1 Creating Derived Features

```python
# Rooms per household
housing["rooms_per_household"] = housing["total_rooms"] / housing["households"]

# Bedrooms per room ratio
housing["bedrooms_per_room"] = housing["total_bedrooms"] / housing["total_rooms"]

# Population per household
housing["population_per_household"] = housing["population"] / housing["households"]
```

**New Features Correlation Analysis:**

| New Feature | Correlation with House Value | Insight |
|-------------|------------------------------|---------|
| `bedrooms_per_room` | **-0.260** | ğŸ”¥ Negative - More bedrooms/room = Lower value |
| `rooms_per_household` | **+0.146** | Positive - More rooms = Higher value |
| `population_per_household` | -0.072 | Weak negative correlation |

**Why `bedrooms_per_room` matters:**
- Lower ratio suggests more spacious homes (living rooms, studies, etc.)
- High ratio indicates compact, basic housing
- Strong negative correlation makes it a valuable feature!

---

### 6. Model Building

Building a complete preprocessing and training pipeline.

#### 6.1 Complete Preprocessing Pipeline

```python
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder

# Separate numerical and categorical features
num_features = housing.select_dtypes(include=[np.number]).columns.tolist()
cat_features = ["ocean_proximity"]

# Numerical pipeline
num_pipeline = Pipeline([
    ('imputer', SimpleImputer(strategy="median")),
    ('scaler', StandardScaler())
])

# Categorical pipeline
cat_pipeline = Pipeline([
    ('imputer', SimpleImputer(strategy="most_frequent")),
    ('encoder', OneHotEncoder(handle_unknown="ignore", sparse_output=False))
])

# Combine pipelines
preprocessor = ColumnTransformer([
    ("num", num_pipeline, num_features),
    ("cat", cat_pipeline, cat_features)
])
```

**Pipeline Benefits:**
- âœ… Automates all preprocessing steps
- âœ… Prevents data leakage (fit only on training data)
- âœ… Ensures consistency between training and inference
- âœ… Easy to deploy and maintain

#### 6.2 Model Training

We trained and compared three different regression models:

##### **Model 1: Linear Regression**

```python
from sklearn.linear_model import LinearRegression
from sklearn.metrics import root_mean_squared_error

lin_reg = LinearRegression()
lin_reg.fit(housing_prepared, housing_labels)

# Predictions on training set
housing_predictions = lin_reg.predict(housing_prepared)
lin_rmse = root_mean_squared_error(housing_labels, housing_predictions)
print(f"Linear Regression RMSE: {lin_rmse}")
```

**Results:**
- **Training RMSE: $69,050**
- Simple model with good interpretability
- Underfits the data (high bias)

##### **Model 2: Decision Tree Regressor**

```python
from sklearn.tree import DecisionTreeRegressor

tree_reg = DecisionTreeRegressor(random_state=42)
tree_reg.fit(housing_prepared, housing_labels)

housing_predictions = tree_reg.predict(housing_prepared)
tree_rmse = root_mean_squared_error(housing_labels, housing_predictions)
print(f"Decision Tree RMSE: {tree_rmse}")
```

**Results:**
- **Training RMSE: $0** (Perfect fit!)
- âš ï¸ **Severe overfitting** - Memorized training data
- Will perform poorly on unseen data

##### **Model 3: Random Forest Regressor** â­ (Selected Model)

```python
from sklearn.ensemble import RandomForestRegressor

forest_reg = RandomForestRegressor(
    n_estimators=100,
    random_state=42,
    n_jobs=-1
)
forest_reg.fit(housing_prepared, housing_labels)

housing_predictions = forest_reg.predict(housing_prepared)
forest_rmse = root_mean_squared_error(housing_labels, housing_predictions)
print(f"Random Forest RMSE: {forest_rmse}")
```

**Results:**
- **Training RMSE: $18,342**
- Excellent balance between bias and variance
- âœ… **Selected as final model**

---

### 7. Model Evaluation

#### 7.1 Cross-Validation

Training error alone can be misleading. Cross-validation provides a more realistic performance estimate.

```python
from sklearn.model_selection import cross_val_score

# 10-Fold Cross-Validation
def evaluate_model(model, X, y):
    scores = -cross_val_score(
        model,
        X,
        y,
        scoring="neg_root_mean_squared_error",
        cv=10
    )
    return scores

# Evaluate all models
lin_scores = evaluate_model(lin_reg, housing_prepared, housing_labels)
tree_scores = evaluate_model(tree_reg, housing_prepared, housing_labels)
forest_scores = evaluate_model(forest_reg, housing_prepared, housing_labels)
```

**Cross-Validation Results:**

| Model | Mean RMSE | Std Dev | Min RMSE | Max RMSE |
|-------|-----------|---------|----------|----------|
| **Linear Regression** | $69,052 | $2,731 | $64,912 | $73,104 |
| **Decision Tree** | $69,081 | $2,420 | $64,770 | $73,280 |
| **Random Forest** | **$47,730** | **$1,892** | **$44,521** | **$50,892** |

**ğŸ“Š Performance Comparison Chart:**

![Model Comparison](performance_comparison.png)

**Key Insights:**

âœ… **Random Forest is the clear winner:**
- 30% better than Linear Regression
- 30% better than Decision Tree
- Most consistent performance (lowest std dev)

âš ï¸ **Decision Tree overfitting confirmed:**
- Training RMSE: $0
- CV RMSE: $69,081
- Massive gap indicates overfitting

#### 7.2 Final Model Performance

**Test Set Evaluation:**

```python
# Prepare test set
X_test = strat_test_set.drop("median_house_value", axis=1)
y_test = strat_test_set["median_house_value"].copy()

# Transform test data
X_test_prepared = preprocessor.transform(X_test)

# Predict
final_predictions = forest_reg.predict(X_test_prepared)
final_rmse = root_mean_squared_error(y_test, final_predictions)

print(f"Test Set RMSE: ${final_rmse:,.2f}")
```

**Final Test Results:**
- **Test RMSE: ~$47,000 - $50,000**
- **Prediction Accuracy: Â±$47k-50k on average**
- **RÂ² Score: ~0.81** (Explains 81% of variance)

**Real-World Performance:**
- For a $200,000 house, prediction error is Â±$47k-50k
- For a $500,000 house, prediction error is Â±$47k-50k
- Acceptable performance for real estate valuation

---

### 8. Model Persistence

Saving the trained model and pipeline for future use.

#### 8.1 Saving with Joblib

```python
import joblib

# Create complete pipeline with model
final_pipeline = Pipeline([
    ("preprocess", preprocessor),
    ("model", forest_reg)
])

# Train on full training set
final_pipeline.fit(X_train, y_train)

# Save pipeline
joblib.dump(final_pipeline, "pipeline.pkl", compress=3)
print("âœ… Model saved successfully!")
```

**Why Joblib?**
- âœ… Efficient for large NumPy arrays
- âœ… Faster than pickle for scikit-learn objects
- âœ… Compression reduces file size
- âœ… Easy to load and use

#### 8.2 Loading and Using Saved Model

```python
# Load saved pipeline
loaded_pipeline = joblib.load("pipeline.pkl")

# Make predictions on new data
predictions = loaded_pipeline.predict(new_data)
```

**Benefits:**
- No need to retrain every time
- Consistent preprocessing and prediction
- Production-ready deployment
- Version control friendly

---

### 9. Web Application

Interactive Streamlit application for real-time predictions.

#### 9.1 Application Features

**ğŸ¨ User Interface:**
- Clean, intuitive design
- Input fields for all features
- Real-time predictions
- Sample dataset viewer

**ğŸ”§ Technical Features:**
- Model loaded from Hugging Face Hub
- Automatic input validation
- Responsive layout
- Caching for performance

#### 9.2 Application Code

```python
import streamlit as st
import pandas as pd
import joblib
from huggingface_hub import hf_hub_download

# Page configuration
st.set_page_config(
    page_title="House Value Prediction",
    layout="centered"
)

st.title("ğŸ  House Value Prediction App")
st.write("Predict house prices using a trained ML pipeline")

# Load model from Hugging Face
@st.cache_resource
def load_pipeline():
    model_path = hf_hub_download(
        repo_id="Subodhit/model",
        filename="pipeline.pkl"
    )
    return joblib.load(model_path)

pipeline = load_pipeline()

# Create input form
st.subheader("Enter House Details")

# Collect user inputs
user_input = {}

# Categorical input
user_input["ocean_proximity"] = st.selectbox(
    "Ocean Proximity",
    ["<1H OCEAN", "INLAND", "NEAR OCEAN", "NEAR BAY", "ISLAND"]
)

# Numerical inputs
user_input["longitude"] = st.number_input("Longitude", value=-122.0)
user_input["latitude"] = st.number_input("Latitude", value=37.0)
user_input["housing_median_age"] = st.number_input("Housing Median Age", value=25)
user_input["total_rooms"] = st.number_input("Total Rooms", value=2000)
user_input["total_bedrooms"] = st.number_input("Total Bedrooms", value=400)
user_input["population"] = st.number_input("Population", value=1000)
user_input["households"] = st.number_input("Households", value=400)
user_input["median_income"] = st.number_input("Median Income (in $10k)", value=3.5)

# Convert to DataFrame
input_df = pd.DataFrame([user_input])

# Make prediction
if st.button("Predict House Value"):
    prediction = pipeline.predict(input_df)[0]
    st.success(f"ğŸ’° Predicted House Value: **${prediction:,.2f}**")
```

#### 9.3 Application Screenshots

**APP LINK:**
[APP LINK](https://house-value-prediction-bysubodhit.streamlit.app/)

**Main Interface:**
![App Main Screen](App_Start.png)

**Prediction Result:**
![Prediction Result](App_Prediction.png)

---

## ğŸš€ Installation & Setup

### Prerequisites

- Python 3.8 or higher
- pip package manager
- Git (for cloning repository)

### Step 1: Clone Repository

```bash
git clone https://github.com/Subodhitchouhan/House-Value-Prediction.git
cd House-Value-Prediction
```

### Step 2: Create Virtual Environment (Recommended)

```bash
# Windows
python -m venv venv
venv\Scripts\activate

# Linux/Mac
python3 -m venv venv
source venv/bin/activate
```

### Step 3: Install Dependencies

```bash
pip install -r requirements.txt
```

**Required Packages:**
```
pandas>=1.5.0
numpy>=1.23.0
scikit-learn>=1.2.0
streamlit>=1.25.0
matplotlib>=3.6.0
seaborn>=0.12.0
joblib>=1.2.0
huggingface-hub>=0.16.0
```

### Step 4: Run Jupyter Notebooks (Optional)

```bash
jupyter notebook
```

Explore the notebooks in order:
1. `1_Analyzing_the_Data.ipynb`
2. `2_Creating_a_Test_Set.ipynb`
3. `3_Stratified_Shuffle_Split.ipynb`
4. `4_Visualizing_The_DaTA.ipynb`
5. `5_Further_Preprocessing.ipynb`
6. `6_Handling_categorical_and_text.ipynb`
7. `7_Feature_Scaling.ipynb`
8. `8_Transformation_Pipelline.ipynb`
9. `9_Training_And_Evaluating_ML_Models.ipynb`
10. `10_Model_Persistence_and_Inference_with_Joblib.ipynb`

### Step 5: Run Streamlit App

```bash
streamlit run app.py
```

The app will open in your browser at `http://localhost:8501`

---

## ğŸ’» Usage

### Using the Web Application

1. **Enter House Details:**
   - Fill in all required fields
   - Use realistic values based on California housing data
   - Select appropriate ocean proximity category

2. **Get Prediction:**
   - Click "Predict House Value" button
   - View predicted price instantly
   - Explore sample dataset for reference values

### Using the Model Programmatically

```python
import pandas as pd
import joblib

# Load the saved pipeline
pipeline = joblib.load("pipeline.pkl")

# Create sample input
sample_data = pd.DataFrame({
    'longitude': [-122.23],
    'latitude': [37.88],
    'housing_median_age': [41],
    'total_rooms': [880],
    'total_bedrooms': [129],
    'population': [322],
    'households': [126],
    'median_income': [8.3252],
    'ocean_proximity': ['NEAR BAY']
})

# Make prediction
prediction = pipeline.predict(sample_data)
print(f"Predicted House Value: ${prediction[0]:,.2f}")
```

**Output:**
```
Predicted House Value: $452,600.00
```

---

## ğŸ“ Project Structure

```
House-Value-Prediction/
â”‚
â”œâ”€â”€ ğŸ““ Notebooks/
â”‚   â”œâ”€â”€ 1_Analyzing_the_Data.ipynb
â”‚   â”œâ”€â”€ 2_Creating_a_Test_Set.ipynb
â”‚   â”œâ”€â”€ 3_Stratified_Shuffle_Split.ipynb
â”‚   â”œâ”€â”€ 4_Visualizing_The_DaTA.ipynb
â”‚   â”œâ”€â”€ 5_Further_Preprocessing.ipynb
â”‚   â”œâ”€â”€ 6_Handling_categorical_and_text.ipynb
â”‚   â”œâ”€â”€ 7_Feature_Scaling.ipynb
â”‚   â”œâ”€â”€ 8_Transformation_Pipelline.ipynb
â”‚   â”œâ”€â”€ 9_Training_And_Evaluating_ML_Models.ipynb
â”‚   â””â”€â”€ 10_Model_Persistence_and_Inference_with_Joblib.ipynb
â”‚
â”œâ”€â”€ ğŸ“Š Data/
â”‚   â””â”€â”€ housing.csv
â”‚
â”œâ”€â”€ ğŸ¤– Models/
â”‚   â””â”€â”€ pipeline.pkl
â”‚
â”œâ”€â”€ ğŸŒ App/
â”‚   â””â”€â”€ app.py
â”‚
â”œâ”€â”€ ğŸ“‹ Requirements/
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ ğŸ“– Documentation/
â”‚   â””â”€â”€ README.md
â”‚
â””â”€â”€ ğŸ“œ License
    â””â”€â”€ LICENSE
```

---

## ğŸ› ï¸ Technologies Used

### Core Technologies

| Technology | Purpose | Version |
|------------|---------|---------|
| **Python** | Programming Language | 3.8+ |
| **Pandas** | Data Manipulation | 1.5.0+ |
| **NumPy** | Numerical Computing | 1.23.0+ |
| **Scikit-Learn** | Machine Learning | 1.2.0+ |
| **Matplotlib** | Data Visualization | 3.6.0+ |
| **Seaborn** | Statistical Visualization | 0.12.0+ |
| **Streamlit** | Web Application | 1.25.0+ |
| **Joblib** | Model Persistence | 1.2.0+ |

### Machine Learning Algorithms

- **Linear Regression** - Baseline model
- **Decision Tree Regressor** - Non-linear model
- **Random Forest Regressor** - Ensemble model (Final choice)

### Preprocessing Techniques

- **SimpleImputer** - Missing value handling
- **StandardScaler** - Feature scaling
- **OneHotEncoder** - Categorical encoding
- **Pipeline** - Workflow automation
- **ColumnTransformer** - Feature-specific transformations

### Deployment

- **Hugging Face Spaces** - Model hosting
- **Streamlit Cloud** - Application deployment
- **GitHub** - Version control

---

## ğŸ“ˆ Results

### Model Performance Summary

| Metric | Value | Description |
|--------|-------|-------------|
| **Training RMSE** | $18,342 | Error on training data |
| **Cross-Validation RMSE** | $47,730 | Average error across 10 folds |
| **Test RMSE** | ~$47,000-50,000 | Error on unseen test data |
| **RÂ² Score** | 0.81 | Explains 81% of variance |
| **Training Time** | ~2 minutes | On standard CPU |

### Model Comparison

```
Linear Regression:    RMSE = $69,052  âŒ High Error
Decision Tree:        RMSE = $69,081  âŒ Overfitting
Random Forest:        RMSE = $47,730  âœ… Best Performance
```

### Key Achievements

âœ… **30% improvement** over baseline Linear Regression
âœ… **Robust performance** with low variance across folds
âœ… **Production-ready** pipeline with automated preprocessing
âœ… **Deployed web app** for real-time predictions
âœ… **Model interpretability** through feature importance analysis

### Feature Importance

Top 5 most important features for prediction:

1. **median_income** (52.3%) - Strongest predictor
2. **longitude** (12.1%) - Geographic importance (West Coast premium)
3. **latitude** (11.2%) - Geographic importance (Southern vs Northern CA)
4. **housing_median_age** (6.8%) - Property age factor
5. **bedrooms_per_room** (4.9%) - Space efficiency indicator

---

## ğŸ”® Future Improvements

### Short-term Enhancements

- [ ] **Hyperparameter tuning** using GridSearchCV or RandomizedSearchCV
- [ ] **Additional feature engineering** (distance to major cities, school ratings)
- [ ] **Ensemble methods** (Gradient Boosting, XGBoost, LightGBM)
- [ ] **Model explainability** (SHAP values, LIME)
- [ ] **Prediction intervals** for uncertainty quantification

### Long-term Goals

- [ ] **Real-time data integration** (Zillow API, Redfin data)
- [ ] **Deep learning models** (Neural networks for non-linear patterns)
- [ ] **Interactive maps** (Folium for geographical predictions)
- [ ] **Mobile application** (React Native or Flutter)
- [ ] **A/B testing framework** for model comparison
- [ ] **Continuous learning** (Model retraining pipeline)

### Technical Debt

- [ ] Add comprehensive unit tests
- [ ] Implement logging and monitoring
- [ ] Create Docker containerization
- [ ] Set up CI/CD pipeline
- [ ] Add API endpoints (FastAPI/Flask)
- [ ] Improve error handling and validation

---

## ğŸ¤ Contributing

Contributions are welcome! Here's how you can help:

### How to Contribute

1. **Fork the repository**
   ```bash
   git fork https://github.com/Subodhitchouhan/House-Value-Prediction.git
   ```

2. **Create a feature branch**
   ```bash
   git checkout -b feature/AmazingFeature
   ```

3. **Make your changes**
   - Follow PEP 8 style guide
   - Add comments and documentation
   - Update README if needed

4. **Commit your changes**
   ```bash
   git commit -m "Add some AmazingFeature"
   ```

5. **Push to the branch**
   ```bash
   git push origin feature/AmazingFeature
   ```

6. **Open a Pull Request**
   - Describe your changes
   - Link related issues
   - Wait for review

### Contribution Guidelines

- Write clear, concise commit messages
- Add tests for new features
- Update documentation
- Follow existing code style
- Be respectful and constructive

---

## ğŸ“„ License

This project is licensed under the **MIT License** - see the [LICENSE](LICENSE) file for details.

```
MIT License

Copyright (c) 2024 Subodhit Chouhan

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.
```

---

## ğŸ“ Contact

**Subodhit Chouhan**

- ğŸ“§ Email: chouhansubodhit@gmail.com
- ğŸ’¼ LinkedIn: https://www.linkedin.com/in/subodhitchouhan/
- ğŸ± GitHub: https://github.com/Subodhitchouhan
- ğŸŒ Portfolio: [yourportfolio.com](https://yourportfolio.com)

---

## ğŸ™ Acknowledgments

- **Dataset Source:** California Housing Prices Dataset
- **Inspiration:** "Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow" by AurÃ©lien GÃ©ron
- **Libraries:** Scikit-Learn, Pandas, Streamlit communities
- **Deployment:** Hugging Face Spaces team

---

## â­ Show Your Support

If this project helped you learn or build something cool, please consider:

- â­ **Starring the repository**
- ğŸ´ **Forking for your own projects**
- ğŸ“¢ **Sharing with others**
- ğŸ› **Reporting bugs or suggesting features**

---

<div align="center">

**Made with â¤ï¸ by Subodhit Chouhan**

[â¬† Back to Top](#-house-value-prediction-app)

</div>
